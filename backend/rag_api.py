# rag_api.py
import os
import shutil
import time
import subprocess
import requests
import torch
from pathlib import Path
from fastapi import FastAPI, HTTPException, Body
from fastapi.middleware.cors import CORSMiddleware # Important pentru comunicarea cu Next.js
from pydantic import BaseModel
import uvicorn

# Importuri Langchain - asigurÄƒ-te cÄƒ sunt corecte È™i complete
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import OllamaLLM
from langchain_core.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_core.documents import Document # Necesar pentru serializare È™i tipare

# === CONFIG === (pÄƒstreazÄƒ configuraÈ›iile tale din scriptul original)
DOCUMENT_FOLDER = Path("./fisiere") # Presupune cÄƒ folderul 'fisiere' e lÃ¢ngÄƒ acest script
CHROMA_DB_PATH = Path("./chroma_db_api") # PoÈ›i folosi un alt nume pentru a nu suprascrie DB-ul original dacÄƒ vrei
CHUNK_SIZE = 256
CHUNK_OVERLAP = 25
K_CONTEXT_CHUNKS = 10
OLLAMA_MODEL_NAME = "llama3.1:8b-instruct-q4_K_S" # AsigurÄƒ-te cÄƒ modelul e corect
OLLAMA_API_URL = "http://localhost:11434"

# --- IniÈ›ializare GlobalÄƒ (se executÄƒ o singurÄƒ datÄƒ la pornirea API-ului) ---
print("--- IniÈ›ializare API RAG Cod Rutier ---")

# È˜TERGE DB EXISTENTÄ‚ (poate vrei sÄƒ controlezi asta diferit Ã®ntr-un API, ex. doar la prima rulare)
if CHROMA_DB_PATH.exists():
    print(f"ğŸ—‘ï¸ È˜terge baza Chroma existentÄƒ din '{CHROMA_DB_PATH}'...")
    shutil.rmtree(CHROMA_DB_PATH)

# PORNEÈ˜TE/VERIFICÄ‚ OLLAMA
def ensure_ollama_model_running(model_name):
    print(f"ğŸ” Verific Ollama È™i modelul `{model_name}`...")
    try:
        r = requests.post(f"{OLLAMA_API_URL}/api/generate",
                          json={"model": model_name, "prompt": "Hi", "stream": False, "options": {"num_predict": 1}}, timeout=3)
        if r.status_code == 200:
            try:
                r.json()
                print("âœ… Modelul Ollama ruleazÄƒ.")
                return
            except requests.exceptions.JSONDecodeError:
                print("âš ï¸ Modelul a rÄƒspuns, dar nu cu JSON valid. VerificÄƒm pornirea...")
        elif r.status_code == 404:
            print(f"âŒ Modelul '{model_name}' nu a fost gÄƒsit Ã®n Ollama. Ãncerc sÄƒ Ã®l descarc/rulez (acest lucru poate dura)...")
        else:
            print(f"âš ï¸ Serverul Ollama a rÄƒspuns cu status {r.status_code}. Ãncerc pornirea modelului...")
    except requests.exceptions.RequestException as e:
        print(f"ğŸ”Œ Serverul Ollama nu rÄƒspunde ({e}). Se Ã®ncearcÄƒ pornirea...")

    print(f"ğŸš€ PorneÈ™te modelul '{model_name}' Ã®n Ollama (dacÄƒ nu este deja pornit)...")
    process_flags = 0
    if os.name == 'nt': # Pentru Windows, ruleazÄƒ Ã®ntr-o consolÄƒ nouÄƒ (dacÄƒ e necesar)
        process_flags = subprocess.CREATE_NEW_CONSOLE
    
    # ÃncercÄƒm sÄƒ pornim modelul. DacÄƒ Ollama nu e pornit, comanda 'ollama run' Ã®l va porni.
    # AceastÄƒ parte poate fi problematicÄƒ dacÄƒ Ollama nu e Ã®n PATH sau necesitÄƒ privilegii.
    # Ideal, Ollama ar trebui sÄƒ ruleze deja ca un serviciu separat.
    try:
        # Comanda `ollama run` este blocantÄƒ dacÄƒ modelul nu e descÄƒrcat,
        # sau dacÄƒ Ollama server nu ruleazÄƒ.
        # Pentru un API, e mai bine ca Ollama sÄƒ fie deja un serviciu pornit.
        # Aici, doar verificÄƒm, presupunÃ¢nd cÄƒ utilizatorul gestioneazÄƒ pornirea Ollama.
        print("   (PresupunÃ¢nd cÄƒ serviciul Ollama este activ È™i modelul este disponibil sau se descarcÄƒ)")
        # subprocess.Popen(["ollama", "run", model_name], creationflags=process_flags)
    except FileNotFoundError:
        print("âŒ Comanda 'ollama' nu a fost gÄƒsitÄƒ. AsigurÄƒ-te cÄƒ Ollama este instalat È™i Ã®n PATH.")
        raise RuntimeError("Ollama command not found. Please start Ollama manually.")
    
    # AÈ™teaptÄƒ ca modelul sÄƒ devinÄƒ disponibil
    for i in range(60): # AÈ™teaptÄƒ pÃ¢nÄƒ la 1 minut
        print(f"â³ AÈ™tept ca modelul '{model_name}' sÄƒ fie gata Ã®n Ollama... ({i+1}/60)")
        try:
            r = requests.post(f"{OLLAMA_API_URL}/api/generate",
                              json={"model": model_name, "prompt": "Hi", "stream": False, "options": {"num_predict": 1}}, timeout=2)
            if r.status_code == 200 and "error" not in r.text.lower():
                try:
                    r.json() # VerificÄƒ dacÄƒ rÄƒspunsul e JSON valid
                    print(f"âœ… Modelul '{model_name}' este gata Ã®n Ollama.")
                    return
                except requests.exceptions.JSONDecodeError:
                    pass # ContinuÄƒ sÄƒ Ã®ncerci dacÄƒ JSON e invalid dar status e 200
            elif r.status_code == 404 or "error" in r.text.lower() : # Model not found or other error
                 print(f"   Modelul '{model_name}' Ã®ncÄƒ nu e gata sau e o eroare: {r.text[:100]}")

        except requests.exceptions.RequestException:
            print("   Serviciul Ollama nu rÄƒspunde Ã®ncÄƒ...")
        time.sleep(2) # AÈ™teaptÄƒ 2 secunde Ã®ntre verificÄƒri
    
    print(f"âŒ Timeout la pornirea/verificarea modelului Ollama '{model_name}'. VerificÄƒ manual Ollama.")
    raise RuntimeError(f"Ollama model '{model_name}' not ready after timeout. Please check Ollama logs.")

ensure_ollama_model_running(OLLAMA_MODEL_NAME)

# EMBEDDING
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸ”„ IniÈ›ializeazÄƒ embedding BGE-M3 pe dispozitivul: {device}...")
embedding_model = HuggingFaceEmbeddings(
    model_name="BAAI/bge-m3",
    model_kwargs={'device': device}
)

# VERIFICÄ‚ FOLDER DOCUMENTE
if not DOCUMENT_FOLDER.is_dir() or not any(DOCUMENT_FOLDER.iterdir()):
    print(f"âŒ Directorul '{DOCUMENT_FOLDER}' nu a fost gÄƒsit sau este gol. CreaÈ›i-l È™i adÄƒugaÈ›i fiÈ™iere .txt È™i .pdf.")
    raise FileNotFoundError(f"Document folder '{DOCUMENT_FOLDER}' not found or is empty.")

# ÃNCARCÄ‚ DOCUMENTELE
docs_loaded = []
print(f"ğŸ“‚ Se Ã®ncarcÄƒ documente din '{DOCUMENT_FOLDER}'...")
for fname in os.listdir(DOCUMENT_FOLDER):
    path_str = str(DOCUMENT_FOLDER / fname)
    file_specific_docs = []
    try:
        if fname.lower().endswith(".txt"):
            loader = TextLoader(path_str, encoding="utf-8")
            file_specific_docs = loader.load()
            print(f"  ğŸ‘ ÃncÄƒrcat (TXT): {fname}")
        elif fname.lower().endswith(".pdf"):
            loader = PyPDFLoader(path_str)
            file_specific_docs = loader.load()
            print(f"  ğŸ‘ ÃncÄƒrcat (PDF): {fname} ({len(file_specific_docs)} pagini procesate)")

        if file_specific_docs:
            for doc_item in file_specific_docs:
                doc_item.metadata["source"] = fname # AdaugÄƒ numele fiÈ™ierului Ã®n metadate
            docs_loaded.extend(file_specific_docs)
        elif not (fname.lower().endswith(".txt") or fname.lower().endswith(".pdf")):
            if not os.path.isdir(DOCUMENT_FOLDER / fname): # IgnorÄƒ subfolderele
                print(f"  âš ï¸ Ignorat (tip fiÈ™ier neacceptat): {fname}")
    except Exception as e:
        print(f"  âŒ Eroare la Ã®ncÄƒrcarea fiÈ™ierului {fname}: {e}")

if not docs_loaded:
    print("âŒ Nu s-au gÄƒsit surse de documente valide (.txt, .pdf) Ã®n folder.")
    raise ValueError("No valid documents found to load.")
print(f"âœ… Total documente (inclusiv pagini PDF) Ã®ncÄƒrcate: {len(docs_loaded)}")

# SPLIT
splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
texts = splitter.split_documents(docs_loaded)
print(f"ğŸ“„ Fragmente create: {len(texts)}")

# VECTORSTORE
print(f"ğŸ’¾ Se creeazÄƒ VectorStore Chroma Ã®n '{CHROMA_DB_PATH}'...")
vectorstore = Chroma(
    embedding_function=embedding_model,
    persist_directory=str(CHROMA_DB_PATH) # Chroma va crea directorul dacÄƒ nu existÄƒ
)
BATCH_SIZE_CHROMA = 500 # PoÈ›i ajusta mÄƒrimea batch-ului dacÄƒ ai multe fragmente
for i in range(0, len(texts), BATCH_SIZE_CHROMA):
    batch = texts[i:i + BATCH_SIZE_CHROMA]
    print(f"ğŸ“¦ Se adaugÄƒ batch de fragmente {i//BATCH_SIZE_CHROMA + 1}/{(len(texts) -1)//BATCH_SIZE_CHROMA + 1} ({len(batch)} fragmente) Ã®n ChromaDB...")
    vectorstore.add_documents(batch)
print("âœ… VectorStore creat È™i fragmentele adÄƒugate.")

# LLM
print(f"ğŸ¤– IniÈ›ializeazÄƒ LLM: {OLLAMA_MODEL_NAME}...")
llm = OllamaLLM(
    model=OLLAMA_MODEL_NAME,
    temperature=0.3,
    top_p=0.95,
    repeat_penalty=1.1,
    num_ctx=4096, # Context window size
    num_predict=512 # Max tokens to predict
)

# PROMPTURI (copiazÄƒ-le exact din scriptul tÄƒu original)
map_prompt_template_str = """
Ai mai jos un fragment de text È™i o Ã®ntrebare principalÄƒ.
Sarcina ta este sÄƒ analizezi CU ATENÈšIE fragmentul È™i sÄƒ extragi ORICE informaÈ›ie din ACEST FRAGMENT care ar putea contribui la rÄƒspunsul final pentru Ãntrebarea principalÄƒ.
ConcentreazÄƒ-te pe extragerea exactÄƒ a detaliilor relevante din fragmentul curent. Nu adÄƒuga interpretÄƒri care nu sunt direct susÈ›inute de textul fragmentului. Nu adÄƒuga informaÈ›ii externe.
DacÄƒ fragmentul curent nu conÈ›ine absolut nicio informaÈ›ie relevantÄƒ pentru Ãntrebarea principalÄƒ, rÄƒspunde foarte scurt, de exemplu: "-fragment irelevant pentru Ã®ntrebare-".

Fragment:
{context}

Ãntrebare principalÄƒ (pentru care acest fragment ar putea conÈ›ine o parte din rÄƒspuns):
{question}

InformaÈ›ii extrase din fragmentul de mai sus, relevante pentru Ã®ntrebarea principalÄƒ (sau "-fragment irelevant pentru Ã®ntrebare-" dacÄƒ este cazul):
"""
map_prompt = PromptTemplate.from_template(map_prompt_template_str)

reduce_prompt_template_str = """
CONTEXT: Ai mai jos o serie de extrageri informaÈ›ionale ("RÄƒspunsuri ParÈ›iale") provenite din diferite fragmente ale Codului Rutier, menite sÄƒ rÄƒspundÄƒ la "Ãntrebarea IniÈ›ialÄƒ". Unele extrageri pot fi etichetate ca "-fragment irelevant-" sau pot conÈ›ine negaÈ›ii despre prezenÈ›a informaÈ›iei.

SARCINA TA PRINCIPALÄ‚: SintetizeazÄƒ un rÄƒspuns comparativ FINAL la "Ãntrebarea IniÈ›ialÄƒ", bazÃ¢ndu-te STRICT pe informaÈ›iile AFIRMATIVE È™i FACTUALE extrase Ã®n "RÄƒspunsurile ParÈ›iale".

PAÈ˜I DE URMAT:
1.  IgnorÄƒ complet orice "RÄƒspuns ParÈ›ial" care este marcat explicit ca "-fragment irelevant-" sau care doar neagÄƒ prezenÈ›a informaÈ›iei fÄƒrÄƒ a oferi date concrete.
2.  Pentru FIECARE ASPECT al "ÃntrebÄƒrii IniÈ›iale" (ex. vÃ¢rstÄƒ minimÄƒ, locuri permise):
    a.  AdunÄƒ TOATE informaÈ›iile afirmative È™i factuale specifice acelui aspect, extrase din "RÄƒspunsurile ParÈ›iale" rÄƒmase, CHIAR DACÄ‚ informaÈ›ia apare Ã®ntr-un singur rÄƒspuns parÈ›ial.
    b.  NoteazÄƒ dacÄƒ pentru un vehicul (ex. bicicletÄƒ) existÄƒ informaÈ›ii despre un aspect, iar pentru celÄƒlalt (ex. trotinetÄƒ) nu existÄƒ informaÈ›ii specifice despre ACELAÈ˜I aspect Ã®n extragerile valide.
3.  ComparÄƒ informaÈ›iile adunate pentru fiecare subiect (ex. trotinete vs. biciclete) È™i pentru fiecare aspect.
4.  FormuleazÄƒ un "RÄƒspuns Comparativ Final" care:
    a.  PrezintÄƒ clar asemÄƒnÄƒrile È™i DEOSEBIRILE specifice pentru fiecare aspect cerut Ã®n "Ãntrebarea IniÈ›ialÄƒ". Include toate detaliile extrase.
    b.  Se bazeazÄƒ EXCLUSIV pe detaliile concrete furnizate Ã®n "RÄƒspunsurile ParÈ›iale" valide.
    c.  DacÄƒ pentru un anumit aspect specific (ex. 'locuri permise pentru biciclete Ã®n lipsa pistei') nu existÄƒ informaÈ›ii concrete afirmative Ã®n niciun "RÄƒspuns ParÈ›ial" valid, menÈ›ioneazÄƒ explicit acest lucru pentru acel aspect È™i vehicul.
    d.  Fii cÃ¢t mai complet È™i detaliat posibil pe baza extragerilor.

RÄƒspunsuri ParÈ›iale (rezultate din analiza individualÄƒ a fragmentelor):
{context}

Ãntrebare IniÈ›ialÄƒ:
{question}

RÄƒspuns Comparativ Final (bazat pe analiza È™i sinteza informaÈ›iilor factuale din rÄƒspunsurile parÈ›iale de mai sus):
"""
reduce_prompt = PromptTemplate.from_template(reduce_prompt_template_str)

prompt_stuff_template_str = """
Folosind strict informaÈ›iile din contextul de mai jos, oferÄƒ un rÄƒspuns cÃ¢t mai complet È™i detaliat posibil la Ã®ntrebare, bazÃ¢ndu-te pe toate detaliile relevante gÄƒsite.
Nu inventa nimic.

Context:
{context}

Ãntrebare: {question}
RÄƒspuns:
"""
prompt_stuff = PromptTemplate.from_template(prompt_stuff_template_str)

# LANÈšURI RAG (RetrievalQA È™i MapReduce)
print("ğŸ”— Se configureazÄƒ lanÈ›urile RAG...")
retriever = vectorstore.as_retriever(search_kwargs={"k": K_CONTEXT_CHUNKS})
map_chain = map_prompt | llm
reduce_chain = reduce_prompt | llm

qa_chain_stuff = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff", # FoloseÈ™te direct contextul recuperat
    chain_type_kwargs={"prompt": prompt_stuff},
    return_source_documents=True # Important pentru a putea afiÈ™a sursele
)
print("âœ… IniÈ›ializare completÄƒ. API-ul este gata sÄƒ primeascÄƒ cereri.")
# --- SfÃ¢rÈ™it IniÈ›ializare GlobalÄƒ ---


# IniÈ›ializeazÄƒ aplicaÈ›ia FastAPI
app = FastAPI(title="RAG Cod Rutier API")

# Configurare CORS (Cross-Origin Resource Sharing)
# Permite cereri de la frontend-ul tÄƒu Next.js care ruleazÄƒ pe localhost:3000
origins = [
    "http://localhost:3000", # Adresa standard pentru Next.js Ã®n mod dezvoltare
    "http://127.0.0.1:3000", # Uneori e nevoie È™i de aceasta
]
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"], # Permite toate metodele (GET, POST, etc.)
    allow_headers=["*"], # Permite toate headerele
)

# Modele Pydantic pentru validarea datelor de request È™i response
class QuestionRequest(BaseModel):
    question: str

class SourceDocumentResponse(BaseModel): # Nume schimbat pentru claritate
    page_content: str
    metadata: dict
    score: float | None = None # Scorul de similaritate, poate fi None

class QAResponse(BaseModel):
    answer: str
    source_documents: list[SourceDocumentResponse]
    processing_time: float

# Endpoint-ul API pentru a primi Ã®ntrebÄƒri
@app.post("/ask", response_model=QAResponse)
async def ask_question_endpoint(request: QuestionRequest = Body(...)):
    question = request.question
    if not question:
        raise HTTPException(status_code=400, detail="Ãntrebarea nu poate fi goalÄƒ.")

    start_time_request = time.time()
    print(f"\nğŸ” Primit Ã®ntrebare prin API: '{question}'")

    try:
        # Recuperare documente È™i scoruri
        print("   ğŸ” Recuperez fragmente È™i scoruri...")
        docs_and_scores_tuples = vectorstore.similarity_search_with_score(question, k=K_CONTEXT_CHUNKS)
        
        documents_for_processing = [doc_score[0] for doc_score in docs_and_scores_tuples]
        scores_for_processing = [doc_score[1] for doc_score in docs_and_scores_tuples]

        if not documents_for_processing:
            print("   âš ï¸ Nu s-au gÄƒsit fragmente relevante.")
            # Chiar dacÄƒ nu gÄƒsim documente, putem Ã®ncerca sÄƒ rÄƒspundem (LLM-ul ar putea avea cunoÈ™tinÈ›e generale)
            # Sau putem returna un mesaj specific. Aici, lÄƒsÄƒm LLM-ul sÄƒ Ã®ncerce cu context gol, sau returnÄƒm mesaj.
            # Pentru RAG, e mai bine sÄƒ indicÄƒm cÄƒ nu s-a gÄƒsit context.
            processing_time = time.time() - start_time_request
            return QAResponse(
                answer="Nu am gÄƒsit informaÈ›ii relevante Ã®n documentele mele pentru a rÄƒspunde la aceastÄƒ Ã®ntrebare.",
                source_documents=[],
                processing_time=processing_time
            )

        final_answer_text = ""
        processed_source_docs_response = []

        # Decide ce lanÈ› sÄƒ foloseÈ™ti: map_reduce pentru comparaÈ›ii, stuff pentru altele
        if any(keyword in question.lower() for keyword in ["diferenÈ›a", "comparaÈ›ie", "vs", "comparativ", "comparÄƒ"]):
            print(f"   âš™ï¸ Procesez ca Ã®ntrebare comparativÄƒ (map_reduce) cu {len(documents_for_processing)} fragmente...")
            partials = []
            for i, doc_content in enumerate(documents_for_processing):
                print(f"      ğŸ—ºï¸ Map Pasul {i+1}/{len(documents_for_processing)} pentru sursa '{doc_content.metadata.get('source', 'necunoscut')}'...")
                partial_response_text = map_chain.invoke({"context": doc_content.page_content, "question": question})
                partials.append(partial_response_text)
            
            joined_partials = "\n\n---\n\n".join(partials)
            print("      âš™ï¸ Reduce: Sintetizez rÄƒspunsul final...")
            final_answer_text = reduce_chain.invoke({"context": joined_partials, "question": question})
            
            # Pentru map_reduce, toate documentele recuperate sunt considerate surse
            for i, doc in enumerate(documents_for_processing):
                processed_source_docs_response.append(SourceDocumentResponse(
                    page_content=doc.page_content,
                    metadata=doc.metadata,
                    score=scores_for_processing[i] if i < len(scores_for_processing) else None
                ))
        else:
            print(f"   âš™ï¸ Procesez ca Ã®ntrebare directÄƒ (stuff) cu {len(documents_for_processing)} fragmente...")
            # Pentru RetrievalQA cu chain_type="stuff", contextul este format din toate documentele recuperate
            # È™i trimis direct la LLM Ã®mpreunÄƒ cu Ã®ntrebarea.
            # 'result' conÈ›ine rÄƒspunsul, 'source_documents' conÈ›ine documentele folosite de chain.
            qa_result = qa_chain_stuff.invoke({"query": question})
            final_answer_text = qa_result['result']
            
            # AsociazÄƒ scorurile cu documentele returnate de qa_chain_stuff
            # Presupunem cÄƒ ordinea È™i numÄƒrul documentelor returnate de qa_chain_stuff
            # corespund celor din docs_and_scores_tuples (ceea ce ar trebui sÄƒ fie adevÄƒrat
            # dacÄƒ retriever-ul este acelaÈ™i È™i k este acelaÈ™i)
            returned_sources_from_chain = qa_result['source_documents']
            for i, doc in enumerate(returned_sources_from_chain):
                 # CautÄƒ scorul original bazat pe conÈ›inutul paginii, ca o mÄƒsurÄƒ de siguranÈ›Äƒ
                 # sau pur È™i simplu foloseÈ™te scorurile Ã®n ordinea primitÄƒ.
                 # Aici folosim ordinea, presupunÃ¢nd cÄƒ este consistentÄƒ.
                original_score = scores_for_processing[i] if i < len(scores_for_processing) else None

                # VerificÄƒm dacÄƒ putem gÄƒsi documentul original pentru a lua scorul mai sigur
                # Aceasta e o potrivire mai robustÄƒ.
                matching_original_doc_score = None
                for k_doc, k_score in docs_and_scores_tuples:
                    if k_doc.page_content == doc.page_content and k_doc.metadata.get('source') == doc.metadata.get('source'):
                        matching_original_doc_score = k_score
                        break
                
                processed_source_docs_response.append(SourceDocumentResponse(
                    page_content=doc.page_content,
                    metadata=doc.metadata,
                    score=matching_original_doc_score if matching_original_doc_score is not None else original_score
                ))


        processing_time = time.time() - start_time_request
        print(f"   âœ… RÄƒspuns generat Ã®n {processing_time:.2f} secunde.")
        
        return QAResponse(
            answer=final_answer_text,
            source_documents=processed_source_docs_response,
            processing_time=processing_time
        )

    except Exception as e:
        print(f"âŒ Eroare majorÄƒ Ã®n timpul procesÄƒrii API: {e}")
        # Aici poÈ›i adÄƒuga logging mai detaliat al excepÈ›iei dacÄƒ e nevoie
        # import traceback
        # print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"Eroare internÄƒ la procesarea Ã®ntrebÄƒrii: {str(e)}")


# AceastÄƒ parte permite rularea serverului direct cu "python rag_api.py"
# Dar este recomandat sÄƒ foloseÈ™ti "uvicorn rag_api:app --reload --port 8000" pentru dezvoltare
if __name__ == "__main__":
    print("ğŸš€ Pornesc serverul API FastAPI cu Uvicorn pe http://localhost:8000")
    print("   Pentru dezvoltare, este recomandat sÄƒ rulezi cu: uvicorn rag_api:app --reload --port 8000")
    # AsigurÄƒ-te cÄƒ Ollama ruleazÄƒ È™i modelul e descÄƒrcat.
    # AsigurÄƒ-te cÄƒ folderul ./fisiere existÄƒ È™i conÈ›ine documente.
    uvicorn.run(app, host="0.0.0.0", port=8000)